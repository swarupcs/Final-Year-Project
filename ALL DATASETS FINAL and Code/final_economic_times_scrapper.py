# -*- coding: utf-8 -*-
"""Final economic times scrapper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zhCQVcweMUyFUk3goRd54PRMY-KMuN3f
"""

!pip install requests
!pip install bs4
!pip install html
!pip install json

import pandas as pd
import requests
import csv
import re
import json
import bs4
import html

def get_blog_url(sp, url_):
  div = sp.find_all('div',attrs={'class':'eachStory'})
  url_list = []
  for h3 in div:
    href = h3.find('a')['href']
    url_list.append(url_+href)
  print(url_list)

  return url_list

def get_blog_content(url):
  req = requests.get(url)
  sp = bs4.BeautifulSoup(req.text, 'html.parser')
  scr = sp.find_all('script', attrs = {'type':'application/ld+json'})
  article = scr[1].get_text().replace("\r\n", ' ')
  article = html.unescape(article)
  parts = re.split(r"""("[^"]*"|'[^']*')""", article)
  article_str = "".join(parts)
  article_dict = json.loads(article_str)
  return article_dict



def save_company_data(url_ = "https://economictimes.indiatimes.com/markets/stocks/news/", sc_id=[], page_no=1, next=0, years=[]):

      url_list = []
      url = "https://economictimes.indiatimes.com/reliance-industries-ltd/stocksupdate/companyid-13215.cms"
      req = requests.get(url)
      sp = bs4.BeautifulSoup(req.text, 'html.parser')
      url_list = get_blog_url(sp, url_)

      for url in url_list:
        try:
          article_dict = get_blog_content(url)
          print(article_dict['datePublished'])
          print(article_dict['author']['name'])
          print(article_dict['headline'])
          print(article_dict['description'])
          print(article_dict['articleBody'])
          print(article_dict['url'])
          print('----------------------------')

          article_lst = [['RI', article_dict['datePublished'],
                                article_dict['author']['name'],
                                article_dict['headline'],
                                article_dict['description'],
                                article_dict['articleBody'],
                                url]]

          with open('economictimes_'+'RI'+'.csv','a',newline='') as file:
                  csv_writer = csv.writer(file)
                  csv_writer.writerows(article_lst)
        except:
          continue
      with open('economictimes_'+'RI'+'.csv', 'r', newline='') as file:
          rows = list(csv.reader(file))
      rows.insert(0,['Company','Date of Publishing','Author','Headline','Description','Article Body','URL'])
      with open('economictimes'+'RI'+'.csv', 'w', newline='') as file:
          writer = csv.writer(file)
          writer.writerows(rows)



save_company_data()

