# -*- coding: utf-8 -*-
"""moneycontrol scrapper final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UVkZLH7s69l5bsV7WPsLZrXFmc7z16z1
"""

import pandas as pd
import requests
import csv
import re
import json
import bs4
import html

def get_blog_url(sp):
  
  anchor = sp.find_all('a', attrs = {'class':'arial11_summ'})
  url_list = []
  
  for attr in anchor:
    href = attr['href']
    url_list.append("https://www.moneycontrol.com/"+href)
  #print(url_list)
  return url_list

def get_blog_content(url):
  req = requests.get(url)
  sp = bs4.BeautifulSoup(req.text, 'html.parser')
  scr = sp.find_all('script', attrs = {'type':'application/ld+json'})
  article = scr[2].get_text().replace("\r\n", ' ')
  article = html.unescape(article)
  parts = re.split(r"""("[^"]*"|'[^']*')""", article)
  parts[::2] = map(lambda s: "".join(s.split()), parts[::2])
  article_str = "".join(parts)
  article_str = article_str[1:]
  article_str = article_str[:-1]
  article_dict = json.loads(article_str)
  return article_dict

def get_page_no(url, sc_id, page_no, next, year):
  req = requests.get(url)
  sp = bs4.BeautifulSoup(req.text, 'html.parser')
  all_page_no = sp.find_all('div', attrs = {'class':'pages MR10 MT15'})
  page_list = [i.text for i in all_page_no[0].find_all('a')]
  if any(map(str.isdigit, page_list[-1])):
    return int(page_list[-1]), next
  else:
    next = next+1
    page_no = int(page_list[-2])
    url = "https://www.moneycontrol.com/stocks/company_info/stock_news.php?sc_id="+sc_id+"&scat=&pageno="+str(page_no)+"&next="+str(next)+"&durationType=Y&Year="+str(year)+"&duration=1&news_type="
    return get_page_no(url, sc_id, page_no, next, year)

def save_company_data(url_ = "https://www.moneycontrol.com/stocks/company_info/stock_news.php?", sc_id=[], page_no=1, next=0, years=[]):
  for company in sc_id:
    for year in years:
      print('year : ', year)
      print('page_no : ', page_no)
      print('next : ', next)

      url = url_ + "sc_id="+company+"&scat=&pageno="+str(page_no)+"&next="+str(next)+"&durationType=Y&Year="+str(year)+"&duration=1&news_type="
      print('url : ', url)

      max_page_no, max_next = get_page_no(url, company, page_no, next, year)
      max_next += 1

      for i in range(max_next):
        for j in range((i*10)+1, (i*10)+11):
          if j <= max_page_no:
            url_list = []
            url = url_ + "sc_id="+company+"&scat=&pageno="+str(j)+"&next="+str(i)+"&durationType=Y&Year="+str(year)+"&duration=1&news_type="
            req = requests.get(url)
            sp = bs4.BeautifulSoup(req.text, 'html.parser')
            url_list = get_blog_url(sp)

            for url in url_list:
              try:
                article_dict = html.unescape(get_blog_content(url))

                print(company)
                print(article_dict['datePublished'])
                print(article_dict['author']['name'])
                print(article_dict['headline'])
                print(article_dict['description'])
                print(article_dict['articleBody'])
                print(article_dict['url'])
                print('----------------------------')

                article_lst = [[company,
                                article_dict['datePublished'],
                                article_dict['author']['name'],
                                article_dict['headline'],
                                article_dict['description'],
                                article_dict['articleBody'],
                                url]]

                with open('moneycontrol_'+company+'.csv','a',newline='') as file:
                  csv_writer = csv.writer(file)
                  csv_writer.writerows(article_lst)
              except:
                continue
            else:
              break
    with open('moneycontrol_'+company+'.csv', 'r', newline='') as file:
       rows = list(csv.reader(file))
    rows.insert(0,['Company','Date of Publishing','Author','Headline','Description','Article Body','URL'])
    with open('moneycontrol_'+company+'.csv', 'w', newline='') as file:
       writer = csv.writer(file)
       writer.writerows(rows)


save_company_data(sc_id=["RI"],years = [2024, 2023, 2022, 2021, 2020])